{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intake: a prototype API for \"data virtualization\"\n",
    "\n",
    "https://github.com/ContinuumIO/intake\n",
    "\n",
    "Access to data has always been a strong suit of Python: many sources of data, whether on-disk file formats or remote databases, have Python libraries (of varying degrees of support, to be honest) to load them.  What has been missing is a consistent interface to these libraries that is suitable for data analytics.\n",
    "\n",
    "The Python DB API interface is fine for SQL-like transactional databases, where iterating over query result elements is desirable.  However, the DB API interface is far too slow for bulk loading of data, which has lead to specific projects (like the old IOPro) to speed up loading data into efficient containers, like NumPy arrays and Pandas dataframes, with a variety of inconsistent interfaces.\n",
    "\n",
    "\"Intake\" is a placeholder name for a project that will address a specific set of read-only usecases that we've seen many times:\n",
    " * Users want to quickly get data from a data source into memory in a familiar container.\n",
    " * Users want to be able to use a \"catalog\" to discover and load pre-defined data sources with a consistent interface, regardless of the underlying data storage system.\n",
    " * Users want to be able to efficiently load data from many sources into distributed computation systems like Dask.\n",
    " \n",
    "Toward this goal, this notebook demostrates a very crude prototype of this functionality to get a sense for how it \"feels\" to use a system like this.\n",
    "\n",
    "## Anti-Goals\n",
    "\n",
    "Intake is **not**:\n",
    " * A complete API for interacting with any data store:  Intake is designed to facilitate bulk loading of data into memory and the creation of data catalogs exclusively.\n",
    " * A replacement for Blaze: Intake could be thought of as `[Blaze] - [Blaze Expressions] + [Dask I/O use cases]`\n",
    " * A tool for writing to data stores\n",
    " * \"One I/O project to rule them all\": The plugin system encourages Intake compatibility code for data sources to exist outside of Intake as separate installable projects, and/or eventually parts of the Python data source libraries themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plugins\n",
    "\n",
    "Intake maintains a registry of reader plugins that support the data loader interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'csv': <intake.plugins.csv.Plugin at 0x10b1ee400>,\n",
       " 'hdf5': <intake.plugins.hdf5.Plugin at 0x10b1ee4a8>,\n",
       " 'postgres': <intake.plugins.postgres.Plugin at 0x10b1ee550>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plugins delegate the actual work to existing libraries, and only function as a shim to provide a consistent interface.  Here is a \"short\" list of plugins that could be easily made, or we need to deliver for customers:\n",
    " * `csv`\n",
    " * `hdf5`\n",
    " * `parquet`\n",
    " * `avro`\n",
    " * `arrow`\n",
    " * `netflow` (Cisco record format)\n",
    " * `pcap` (package capture format produced by tcpdump, etc)\n",
    " * `postgres`\n",
    " * `mongo`\n",
    " * `accumulo`\n",
    " * `odbc`\n",
    " * `elasticsearch`\n",
    " * `solr`\n",
    " * `splunk`\n",
    " * `hbase`\n",
    "\n",
    "Plugins can be accessed through the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<intake.plugins.csv.CSVSource at 0x113da5b38>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.registry['csv'].open('data/trip_data_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but for convenience, `intake` automatically creates a shortcut to the `open()` method on each plugin called `open_[plugin_name]()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<intake.plugins.csv.CSVSource at 0x113da5588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.open_csv('data/trip_data_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with data sources\n",
    "\n",
    "Every plugin provides an `open` method that returns a `DataSource` object, representing a \"lazy\" reference to the data source.  The object will synchronously open files, initiate database connection, inspect data types, but will not load data into memory until asked.\n",
    "\n",
    "The `DataSource` object that is returned by `open()` provides a simple interface for getting the data into memory using a standard container.  The initially supported containers are likely to be:\n",
    " \n",
    " * `dataframe`: A pandas dataframe.  This is the preferred form for tabular data.\n",
    " * `ndarray`: A Numpy array.  This form is used for multidimensional data.\n",
    " * `list`: A list of JSON-like data structures.  This form is used for unstructured/semi-structured data.\n",
    " \n",
    "A given `DataSource` will chose one of these containers to return (see below) as there is usually one preferable choice depending on the structure of the data itself.\n",
    "\n",
    "The attributes of `DataSource` describe the data that will be available before it is loaded (if possible):\n",
    "\n",
    " * `datashape`: (TBD) description of data using datashape\n",
    " * `dtype`: NumPy dtype, if applicable, or `None`\n",
    " * `shape`: NumPy-style shape tuple, if applicable, or `None`.  DataFrames and lists are considered 1D for the purposes of this attribute.  Specific dimensions that are unknown until read time should have a `None` placeholder.\n",
    " * `container`: String describing the container format that will be returned: `dataframe`, `ndarray`, or `list`.\n",
    " * `get_chunks_supported`: True/False flag indicating whether the `get_chunks()` method can return more than one chunk.\n",
    "\n",
    "The `DataSource` object also has two methods:\n",
    "\n",
    " * `read()`: Load all the data into memory at once and return the appropriate container.\n",
    " * `read_chunks(chunksize)`: Returns an iterator over data chunks of approximately `chunksize`.\n",
    " * `get_chunks(chunksize)`: Returns a list of data sources that retrieve individual chunks (as `read_chunks()` would).  If this chunked sources cannot be created (for example, a database query), then a list of length 1 is returned with a `DataSource` equivalent to `self`.\n",
    " \n",
    "Although `DataSource` objects may have open file handles and socket connections, they need to be pickleable for transfer using Dask.  This means that they typically need to implement their own `__setstate__` method to record only pickleable attributes and a `__getstate__` method that can reopen file and network connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `hdf5` - wrapper around h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 311, 223, 3) uint8\n"
     ]
    }
   ],
   "source": [
    "hdf5_reader = intake.open_hdf5('data/images.hdf5', dataset='AER')\n",
    "print(hdf5_reader.shape, hdf5_reader.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 311, 223, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = hdf5_reader.read_chunks(chunksize=10)\n",
    "chunk = next(it)\n",
    "print(chunk.shape)\n",
    "chunk[:2,:2,:3] # a small slice of the chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `csv` - wrapper around `pandas.read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader = intake.open_csv('data/trip_data_*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medallion</th>\n",
       "      <th>hack_license</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_time_in_secs</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n",
       "      <td>BA96DE419E711691B9445D6A6307C170</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-01 15:11:48</td>\n",
       "      <td>2013-01-01 15:18:10</td>\n",
       "      <td>4</td>\n",
       "      <td>382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.978165</td>\n",
       "      <td>40.757977</td>\n",
       "      <td>-73.989838</td>\n",
       "      <td>40.751171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-06 00:18:35</td>\n",
       "      <td>2013-01-06 00:22:54</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-74.006683</td>\n",
       "      <td>40.731781</td>\n",
       "      <td>-73.994499</td>\n",
       "      <td>40.750660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-05 18:49:41</td>\n",
       "      <td>2013-01-05 18:54:23</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-74.004707</td>\n",
       "      <td>40.737770</td>\n",
       "      <td>-74.009834</td>\n",
       "      <td>40.726002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:54:15</td>\n",
       "      <td>2013-01-07 23:58:20</td>\n",
       "      <td>2</td>\n",
       "      <td>244</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-73.974602</td>\n",
       "      <td>40.759945</td>\n",
       "      <td>-73.984734</td>\n",
       "      <td>40.759388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:25:03</td>\n",
       "      <td>2013-01-07 23:34:24</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-73.976250</td>\n",
       "      <td>40.748528</td>\n",
       "      <td>-74.002586</td>\n",
       "      <td>40.747868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          medallion                      hack_license  \\\n",
       "0  89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
       "1  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "2  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "3  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "4  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "\n",
       "  vendor_id  rate_code store_and_fwd_flag      pickup_datetime  \\\n",
       "0       CMT          1                  N  2013-01-01 15:11:48   \n",
       "1       CMT          1                  N  2013-01-06 00:18:35   \n",
       "2       CMT          1                  N  2013-01-05 18:49:41   \n",
       "3       CMT          1                  N  2013-01-07 23:54:15   \n",
       "4       CMT          1                  N  2013-01-07 23:25:03   \n",
       "\n",
       "      dropoff_datetime  passenger_count  trip_time_in_secs  trip_distance  \\\n",
       "0  2013-01-01 15:18:10                4                382            1.0   \n",
       "1  2013-01-06 00:22:54                1                259            1.5   \n",
       "2  2013-01-05 18:54:23                1                282            1.1   \n",
       "3  2013-01-07 23:58:20                2                244            0.7   \n",
       "4  2013-01-07 23:34:24                1                560            2.1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \n",
       "0        -73.978165        40.757977         -73.989838         40.751171  \n",
       "1        -74.006683        40.731781         -73.994499         40.750660  \n",
       "2        -74.004707        40.737770         -74.009834         40.726002  \n",
       "3        -73.974602        40.759945         -73.984734         40.759388  \n",
       "4        -73.976250        40.748528         -74.002586         40.747868  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = csv_reader.read()\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk, size = 70000\n",
      "Chunk, size = 70000\n",
      "Chunk, size = 59999\n",
      "Chunk, size = 70000\n",
      "Chunk, size = 70000\n",
      "Chunk, size = 59999\n"
     ]
    }
   ],
   "source": [
    "for chunk in csv_reader.read_chunks(chunksize=70000):\n",
    "    print('Chunk, size =', len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `postgres` - wrapper around PostgresAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization and get_chunks()\n",
    "\n",
    "To demonstrate the serializability of `DataSource` objects, lets save one to disk and reload it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('csv_reader.pkl', 'wb') as f:\n",
    "    pickle.dump(csv_reader, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medallion</th>\n",
       "      <th>hack_license</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_time_in_secs</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n",
       "      <td>BA96DE419E711691B9445D6A6307C170</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-01 15:11:48</td>\n",
       "      <td>2013-01-01 15:18:10</td>\n",
       "      <td>4</td>\n",
       "      <td>382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.978165</td>\n",
       "      <td>40.757977</td>\n",
       "      <td>-73.989838</td>\n",
       "      <td>40.751171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-06 00:18:35</td>\n",
       "      <td>2013-01-06 00:22:54</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-74.006683</td>\n",
       "      <td>40.731781</td>\n",
       "      <td>-73.994499</td>\n",
       "      <td>40.750660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-05 18:49:41</td>\n",
       "      <td>2013-01-05 18:54:23</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-74.004707</td>\n",
       "      <td>40.737770</td>\n",
       "      <td>-74.009834</td>\n",
       "      <td>40.726002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:54:15</td>\n",
       "      <td>2013-01-07 23:58:20</td>\n",
       "      <td>2</td>\n",
       "      <td>244</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-73.974602</td>\n",
       "      <td>40.759945</td>\n",
       "      <td>-73.984734</td>\n",
       "      <td>40.759388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:25:03</td>\n",
       "      <td>2013-01-07 23:34:24</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-73.976250</td>\n",
       "      <td>40.748528</td>\n",
       "      <td>-74.002586</td>\n",
       "      <td>40.747868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          medallion                      hack_license  \\\n",
       "0  89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
       "1  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "2  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "3  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "4  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "\n",
       "  vendor_id  rate_code store_and_fwd_flag      pickup_datetime  \\\n",
       "0       CMT          1                  N  2013-01-01 15:11:48   \n",
       "1       CMT          1                  N  2013-01-06 00:18:35   \n",
       "2       CMT          1                  N  2013-01-05 18:49:41   \n",
       "3       CMT          1                  N  2013-01-07 23:54:15   \n",
       "4       CMT          1                  N  2013-01-07 23:25:03   \n",
       "\n",
       "      dropoff_datetime  passenger_count  trip_time_in_secs  trip_distance  \\\n",
       "0  2013-01-01 15:18:10                4                382            1.0   \n",
       "1  2013-01-06 00:22:54                1                259            1.5   \n",
       "2  2013-01-05 18:54:23                1                282            1.1   \n",
       "3  2013-01-07 23:58:20                2                244            0.7   \n",
       "4  2013-01-07 23:34:24                1                560            2.1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \n",
       "0        -73.978165        40.757977         -73.989838         40.751171  \n",
       "1        -74.006683        40.731781         -73.994499         40.750660  \n",
       "2        -74.004707        40.737770         -74.009834         40.726002  \n",
       "3        -73.974602        40.759945         -73.984734         40.759388  \n",
       "4        -73.976250        40.748528         -74.002586         40.747868  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('csv_reader.pkl', 'rb') as f:\n",
    "    new_reader = pickle.load(f)\n",
    "    \n",
    "new_reader.read().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason we want this feature is so that we can combine it with `get_chunks()`.  This method attempts to returns a list of `DataSource` objects representing consecutive segments of data covering the original source.  Like `read_chunks()`, the `chunksize` is a suggestion, and actual chunk sizes will vary.\n",
    "\n",
    "Let's chunk up our HDF5 source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<intake.plugins.hdf5.HDF5Source object at 0x119369320>, <intake.plugins.hdf5.HDF5Source object at 0x119369208>, <intake.plugins.hdf5.HDF5Source object at 0x119369cc0>, <intake.plugins.hdf5.HDF5Source object at 0x114ac2f60>, <intake.plugins.hdf5.HDF5Source object at 0x114ac2b00>]\n"
     ]
    }
   ],
   "source": [
    "chunk_sources = hdf5_reader.get_chunks(chunksize=10)\n",
    "print(chunk_sources[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 311, 223, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_sources[1].read().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Catalogs\n",
    "\n",
    "A data catalog is a collection of named catalog entries (effectively `DataSource` objects) that can be accessed by the user.  To allow for some flexibility in the returned data, a catalog entry can take zero or more user parameters, which will modify the query.  This puts some additional work on the catalog administrator to create parameterized queries, but it avoids the need for the full Blaze expression system to filter a data source.\n",
    "\n",
    "The canonical description of a data catalog is a YAML file.  Let's make a catalog for the above examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting catalog.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile catalog.yml\n",
    "taxi_data:\n",
    "  description: NYC Taxi Data\n",
    "  driver: csv\n",
    "  args: # passed to the open() method\n",
    "    file_pattern: data/trip_data_*.csv\n",
    "\n",
    "card_images:\n",
    "  description: Card images\n",
    "  parameters: # User defined parameters\n",
    "    setname:\n",
    "      description: Name of MTG card set\n",
    "      type: str\n",
    "      default: AER\n",
    "      allowed: [\"AER\"]\n",
    "  driver: hdf5\n",
    "  args:\n",
    "    filename: data/images.hdf5\n",
    "    dataset: !template_str \"{{ setname }}\"  # Mark this string as a Jinja2 template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Catalogs\n",
    "\n",
    "A local catalog is one that exists in the client.  This can be useful when you want to declare a bunch of data sources in an external configuration file and use them directly.  One could image Anaconda projects shipping with a catalog describing all the data sources they use.  (The local catalog also becomes the basis of the remote catalog shown later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['taxi_data', 'card_images']\n",
      "{'description': 'Card images', 'user_parameters': [{'name': 'setname', 'description': 'Name of MTG card set', 'type': 'str', 'default': 'AER', 'allowed': ['AER']}]}\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "catalog = intake.load_catalog('catalog.yml')\n",
    "print(catalog.list())\n",
    "print(catalog.describe('card_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsource = catalog.get('taxi_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medallion</th>\n",
       "      <th>hack_license</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_time_in_secs</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n",
       "      <td>BA96DE419E711691B9445D6A6307C170</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-01 15:11:48</td>\n",
       "      <td>2013-01-01 15:18:10</td>\n",
       "      <td>4</td>\n",
       "      <td>382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.978165</td>\n",
       "      <td>40.757977</td>\n",
       "      <td>-73.989838</td>\n",
       "      <td>40.751171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-06 00:18:35</td>\n",
       "      <td>2013-01-06 00:22:54</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-74.006683</td>\n",
       "      <td>40.731781</td>\n",
       "      <td>-73.994499</td>\n",
       "      <td>40.750660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-05 18:49:41</td>\n",
       "      <td>2013-01-05 18:54:23</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-74.004707</td>\n",
       "      <td>40.737770</td>\n",
       "      <td>-74.009834</td>\n",
       "      <td>40.726002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:54:15</td>\n",
       "      <td>2013-01-07 23:58:20</td>\n",
       "      <td>2</td>\n",
       "      <td>244</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-73.974602</td>\n",
       "      <td>40.759945</td>\n",
       "      <td>-73.984734</td>\n",
       "      <td>40.759388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:25:03</td>\n",
       "      <td>2013-01-07 23:34:24</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-73.976250</td>\n",
       "      <td>40.748528</td>\n",
       "      <td>-74.002586</td>\n",
       "      <td>40.747868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          medallion                      hack_license  \\\n",
       "0  89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
       "1  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "2  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "3  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "4  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "\n",
       "  vendor_id  rate_code store_and_fwd_flag      pickup_datetime  \\\n",
       "0       CMT          1                  N  2013-01-01 15:11:48   \n",
       "1       CMT          1                  N  2013-01-06 00:18:35   \n",
       "2       CMT          1                  N  2013-01-05 18:49:41   \n",
       "3       CMT          1                  N  2013-01-07 23:54:15   \n",
       "4       CMT          1                  N  2013-01-07 23:25:03   \n",
       "\n",
       "      dropoff_datetime  passenger_count  trip_time_in_secs  trip_distance  \\\n",
       "0  2013-01-01 15:18:10                4                382            1.0   \n",
       "1  2013-01-06 00:22:54                1                259            1.5   \n",
       "2  2013-01-05 18:54:23                1                282            1.1   \n",
       "3  2013-01-07 23:58:20                2                244            0.7   \n",
       "4  2013-01-07 23:34:24                1                560            2.1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \n",
       "0        -73.978165        40.757977         -73.989838         40.751171  \n",
       "1        -74.006683        40.731781         -73.994499         40.750660  \n",
       "2        -74.004707        40.737770         -74.009834         40.726002  \n",
       "3        -73.974602        40.759945         -73.984734         40.759388  \n",
       "4        -73.976250        40.748528         -74.002586         40.747868  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsource.read().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User parameters to a data source are passed as keyword arguments to `catalog.get()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csource = catalog.get('card_images', setname='AER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Catalogs\n",
    "\n",
    "Remote catalogs are catalogs that communicate over a network connection.  They have two purposes:\n",
    "\n",
    " * Sharing a data catalog with many clients\n",
    " * Proxying a data source to the client without having to share direct access privileges (passwords, disk access, etc)\n",
    " \n",
    "For this prototype, we're using ZeroMQ and pickle to send Python objects.  The real system probably requires something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: taxi_data,card_images\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "catalog = intake.load_catalog('tcp://127.0.0.1:5555')\n",
    "print('Available datasets: ' + ','.join(catalog.list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medallion</th>\n",
       "      <th>hack_license</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_time_in_secs</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89D227B655E5C82AECF13C3F540D4CF4</td>\n",
       "      <td>BA96DE419E711691B9445D6A6307C170</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-01 15:11:48</td>\n",
       "      <td>2013-01-01 15:18:10</td>\n",
       "      <td>4</td>\n",
       "      <td>382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.978165</td>\n",
       "      <td>40.757977</td>\n",
       "      <td>-73.989838</td>\n",
       "      <td>40.751171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-06 00:18:35</td>\n",
       "      <td>2013-01-06 00:22:54</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-74.006683</td>\n",
       "      <td>40.731781</td>\n",
       "      <td>-73.994499</td>\n",
       "      <td>40.750660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0BD7C8F5BA12B88E0B67BED28BEA73D8</td>\n",
       "      <td>9FD8F69F0804BDB5549F40E9DA1BE472</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-05 18:49:41</td>\n",
       "      <td>2013-01-05 18:54:23</td>\n",
       "      <td>1</td>\n",
       "      <td>282</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-74.004707</td>\n",
       "      <td>40.737770</td>\n",
       "      <td>-74.009834</td>\n",
       "      <td>40.726002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:54:15</td>\n",
       "      <td>2013-01-07 23:58:20</td>\n",
       "      <td>2</td>\n",
       "      <td>244</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-73.974602</td>\n",
       "      <td>40.759945</td>\n",
       "      <td>-73.984734</td>\n",
       "      <td>40.759388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DFD2202EE08F7A8DC9A57B02ACB81FE2</td>\n",
       "      <td>51EE87E3205C985EF8431D850C786310</td>\n",
       "      <td>CMT</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>2013-01-07 23:25:03</td>\n",
       "      <td>2013-01-07 23:34:24</td>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>2.1</td>\n",
       "      <td>-73.976250</td>\n",
       "      <td>40.748528</td>\n",
       "      <td>-74.002586</td>\n",
       "      <td>40.747868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          medallion                      hack_license  \\\n",
       "0  89D227B655E5C82AECF13C3F540D4CF4  BA96DE419E711691B9445D6A6307C170   \n",
       "1  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "2  0BD7C8F5BA12B88E0B67BED28BEA73D8  9FD8F69F0804BDB5549F40E9DA1BE472   \n",
       "3  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "4  DFD2202EE08F7A8DC9A57B02ACB81FE2  51EE87E3205C985EF8431D850C786310   \n",
       "\n",
       "  vendor_id  rate_code store_and_fwd_flag      pickup_datetime  \\\n",
       "0       CMT          1                  N  2013-01-01 15:11:48   \n",
       "1       CMT          1                  N  2013-01-06 00:18:35   \n",
       "2       CMT          1                  N  2013-01-05 18:49:41   \n",
       "3       CMT          1                  N  2013-01-07 23:54:15   \n",
       "4       CMT          1                  N  2013-01-07 23:25:03   \n",
       "\n",
       "      dropoff_datetime  passenger_count  trip_time_in_secs  trip_distance  \\\n",
       "0  2013-01-01 15:18:10                4                382            1.0   \n",
       "1  2013-01-06 00:22:54                1                259            1.5   \n",
       "2  2013-01-05 18:54:23                1                282            1.1   \n",
       "3  2013-01-07 23:58:20                2                244            0.7   \n",
       "4  2013-01-07 23:34:24                1                560            2.1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \n",
       "0        -73.978165        40.757977         -73.989838         40.751171  \n",
       "1        -74.006683        40.731781         -73.994499         40.750660  \n",
       "2        -74.004707        40.737770         -74.009834         40.726002  \n",
       "3        -73.974602        40.759945         -73.984734         40.759388  \n",
       "4        -73.976250        40.748528         -74.002586         40.747868  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_source = catalog.get('taxi_data')\n",
    "remote_source.read().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk, size = 70000\n",
      "Chunk, size = 70000\n",
      "Chunk, size = 59999\n",
      "Chunk, size = 70000\n",
      "Chunk, size = 70000\n",
      "Chunk, size = 59999\n"
     ]
    }
   ],
   "source": [
    "for chunk in remote_source.read_chunks(chunksize=70000):\n",
    "    print('Chunk, size =', len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask interoperability\n",
    "\n",
    "One other goal of this system is to provide an easy on-ramp to get data sources into Dask.  The chunking functionality is intended to allow loading and scattering data sources that don't fit into the memory of a single system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 311, 223, 3)\n"
     ]
    }
   ],
   "source": [
    "import intake\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "\n",
    "catalog = intake.load_catalog('catalog.yml')\n",
    "images = catalog.get('card_images')\n",
    "\n",
    "\n",
    "a = intake.to_dask(images, client=client)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Extensions\n",
    "\n",
    "This kind of catalog system has a natural product extensions:\n",
    "\n",
    "* Local catalogs defined in a `catalog.yml` file could be part of an Anaconda Project\n",
    "* Anaconda Enterprise could give users/administrators the ability to create remote data catalogs and set access permissions for the data sources.\n",
    "* Handling of authentication credentials is not described here.\n",
    "* Does a `read()` on a remote data sources have to be proxied through the catalog server?  There are data sources that would be more efficiently accessed directly, so the remote catalog could  pass along the connection information to the client (assuming it had the appropriate plugin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
